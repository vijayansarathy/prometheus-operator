#
# The Prometheus custom resource definition (CRD) declaratively defines a desired Prometheus setup to run in a Kubernetes cluster.
# For each Prometheus custom resource, the Prometheus Operator deploys a properly configured Prometheus server instance as a StatefulSet in the same namespace.
# The Pods of the StatefulSet are configured to mount a Secret containing the configuration for Prometheus server instance.
# The name of this Secret is the same as that of the Prometheus custom resource which triggeres the deployment of the StatefulSet
# The Prometheus custom resource includes a field called "serviceMonitorSelector", which defines a selection of ServiceMonitors to be used.
# ServiceMonitors can be selected outside the Prometheus namespace via the "serviceMonitorNamespaceSelector" field of the Prometheus custom resource.
# The Prometheus Operator then generates a configuration based on the included ServiceMonitors and updates it in the Secret containing the configuration.
# It continuously does so for all changes that are made to ServiceMonitors or the Prometheus resource itself.
#
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  namespace: monitoring
  name: prometheus-k8s
  labels:
    prometheus: k8s  
spec:
  replicas: 1
  serviceAccountName: prometheus
  serviceMonitorNamespaceSelector:
    matchLabels:
      monitor: prometheus
  serviceMonitorSelector:
    matchLabels:
      category: web-service
      monitor: prometheus
  resources:
    requests:
      memory: 400Mi
  enableAdminAPI: true

---
#
# If RBAC authorization is activated, you must create RBAC rules for both Prometheus and Prometheus Operator
# The Prometheus Operator takes care of provisioning/operating Prometheus server instances using a StatefuleSet based on the above Prometheus custom resource
# Note that the StatefuleSet will be named prometheus-<prometheus-name>, where <prometheus-name> is the name assigned to the Prometheus custom resource
# All the Pods in the StatefulSet are labeled with "prometheus: <prometheus-name>"
# The easiest way to expose the Prometheus server instances is to use a Service of type NodePort to front these Pods.
# If the Prometheus Prometheus custom resource's name is "prometheus-k8s", then the respective manifest for the Service must define the selector as "prometheus: prometheus-k8s"
#
apiVersion: v1
kind: Service
metadata:
  namespace: monitoring
  name: prometheus-svc
spec:
  type: NodePort
  ports:
  - name: web
    nodePort: 30900
    port: 9090
    protocol: TCP
    targetPort: web
  selector:
    prometheus: prometheus-k8s

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring

---
# 
# The Prometheus server accesses the Kubernetes API to discover targets and Alertmanagers.
# Therefore a separate ClusterRole for those Prometheus servers needs to exist.
# Note that this is separate from the ClusterRole created for granting the necessary permissions to the Prometheus Operator
# As Prometheus does not modify any Objects in the Kubernetes API, but just reads them it simply requires the get, list, and watch actions.
# As Prometheus can also be used to scrape metrics from the Kubernetes apiserver, it also requires access to the /metrics/ endpoint of it.
# In addition to the resources Prometheus itself needs to access, the Prometheus side-car needs to be able to get configmaps to be able to pull in rule files from configmap objects.
#
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring